{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid19_Detection",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/pavi-ninjaac/SARS_CoV_2_DNA_Analysis/blob/main/Model_training/Covid19_Detection.ipynb",
      "authorship_tag": "ABX9TyPVgPnlAHmLwxva1fnjeSyz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavi-ninjaac/SARS_CoV_2_DNA_Analysis/blob/main/Model_training/Covid19_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH6HWwmNN6H1"
      },
      "source": [
        "# Covid19 Image training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeYt926zN4Vl"
      },
      "source": [
        "#import the libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import json\r\n",
        "import itertools\r\n",
        "import cv2\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "#keras \r\n",
        "from keras.utils.np_utils import to_categorical\r\n",
        "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\r\n",
        "from keras.models import load_model\r\n",
        "from keras.preprocessing import image\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "#keras layers\r\n",
        "from keras.models import Sequential \r\n",
        "from keras.layers import Conv2D,MaxPooling2D,BatchNormalization,AveragePooling2D\r\n",
        "from keras.layers import Flatten,Dropout,Dense"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B55z8vwAUMSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4129f460-aff1-48ec-bb4c-73e7c0801c80"
      },
      "source": [
        "#the data was in the numpy array file (.npy), we need to load from there\r\n",
        "X = np.load('/content/drive/MyDrive/Colab Notebooks/dataFiles/DNA_Genome_Analysis_covid19/generated_FCGR_Images/dna_image_pixel_x.npy')\r\n",
        "y = np.load('/content/drive/MyDrive/Colab Notebooks/dataFiles/DNA_Genome_Analysis_covid19/generated_FCGR_Images/dna_image_pixcel_y.npy')\r\n",
        "X.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1557, 128, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA9L4ZqJH10I"
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X , y , test_size = 0.2 , random_state = 42)\r\n",
        "X_test,X_val , y_test , y_val = train_test_split(X_test,y_test , test_size =0.2 , random_state = 42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoA0HfjKJGmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d4f30d-463a-4318-f0b7-d2859efac2b1"
      },
      "source": [
        "print(\"The size of the train data-------------------->\",X_train.shape)\r\n",
        "print(\"The size of the train target data------------->\",y_train.shape)\r\n",
        "print()\r\n",
        "print(\"The size of the test data--------------------->\",X_test.shape)\r\n",
        "print(\"The size of the test target data-------------->\",y_test.shape)\r\n",
        "print()\r\n",
        "print(\"The size of the validation data--------------->\",X_val.shape)\r\n",
        "print(\"The size of the validation target data-------->\",y_val.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of the train data--------------------> (1245, 128, 128)\n",
            "The size of the train target data-------------> (1245,)\n",
            "\n",
            "The size of the test data---------------------> (249, 128, 128)\n",
            "The size of the test target data--------------> (249,)\n",
            "\n",
            "The size of the validation data---------------> (63, 128, 128)\n",
            "The size of the validation target data--------> (63,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_cmcu21FyfF"
      },
      "source": [
        "\r\n",
        "def clean_data(X_train,y_train,X_test,y_test,X_val,y_val):\r\n",
        "  \r\n",
        "\r\n",
        "  print(\"Collecting all data...................\")\r\n",
        "  \r\n",
        "  print(\"Converting to numpy array>>>>>>>>>>>>>>>>>>\")\r\n",
        "  #convert list to numpy array\r\n",
        "  X_train = np.array(X_train,'float32')  \r\n",
        "  y_train = np.array(y_train,'float32')  \r\n",
        "  X_test = np.array(X_test,'float32')  \r\n",
        "  y_test = np.array(y_test,'float32')\r\n",
        "  X_val = np.array(X_val,'float32')  \r\n",
        "  y_val = np.array(y_val,'float32')\r\n",
        "\r\n",
        "  print(\"Normalizing the data>>>>>>>>>>>>>>>>>>>>>>\")\r\n",
        "  #normalize the data\r\n",
        "  X_train  = X_train/255.0\r\n",
        "  X_test = X_test/255.0\r\n",
        "  X_val = X_val/255.0\r\n",
        "\r\n",
        "  print(\"Converting target to one hot encoded values>>>>>>>>>>>>>>>>>>>>>>\")\r\n",
        "  #convert to numerical values to 0,1\r\n",
        "  y_train = to_categorical(y_train,num_classes=2)\r\n",
        "  y_test = to_categorical(y_test,num_classes=2)\r\n",
        "  y_val = to_categorical(y_val,num_classes=2)\r\n",
        "\r\n",
        "  print(\"reshaping the data>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\r\n",
        "  #reshape the train X data to 48 x 48 picxels\r\n",
        "  #the len of the given picxels is 2304 ------>(48*48)\r\n",
        "  X_train = X_train.reshape(X_train.shape[0] , 128 ,128 , 1)\r\n",
        "  X_test = X_test.reshape(X_test.shape[0] , 128 ,128 , 1)\r\n",
        "  X_val = X_val.reshape(X_val.shape[0] , 128 ,128 , 1)\r\n",
        "\r\n",
        "  print(\"Preprocessing  completed!!!!!!!!!! stay happy :)\")\r\n",
        "  return X_train,y_train,X_test,y_test,X_val,y_val"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQQDM0kIJheR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11611735-08fc-49a6-fc8f-fcdbeb936c3c"
      },
      "source": [
        "X_train,y_train,X_test,y_test,X_val,y_val = clean_data(X_train,y_train,X_test,y_test,X_val,y_val)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting all data...................\n",
            "Converting to numpy array>>>>>>>>>>>>>>>>>>\n",
            "Normalizing the data>>>>>>>>>>>>>>>>>>>>>>\n",
            "Converting target to one hot encoded values>>>>>>>>>>>>>>>>>>>>>>\n",
            "reshaping the data>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Preprocessing  completed!!!!!!!!!! stay happy :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZkSoZn9JqWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6b1509-4402-477f-d507-e6ebed227adb"
      },
      "source": [
        "print(\"The size of the train data-------------------->\",X_train.shape)\r\n",
        "print(\"The size of the train target data------------->\",y_train.shape)\r\n",
        "print()\r\n",
        "print(\"The size of the test data--------------------->\",X_test.shape)\r\n",
        "print(\"The size of the test target data-------------->\",y_test.shape)\r\n",
        "print()\r\n",
        "print(\"The size of the validation data--------------->\",X_val.shape)\r\n",
        "print(\"The size of the validation target data-------->\",y_val.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of the train data--------------------> (1245, 128, 128, 1)\n",
            "The size of the train target data-------------> (1245, 2)\n",
            "\n",
            "The size of the test data---------------------> (249, 128, 128, 1)\n",
            "The size of the test target data--------------> (249, 2)\n",
            "\n",
            "The size of the validation data---------------> (63, 128, 128, 1)\n",
            "The size of the validation target data--------> (63, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKPaZ6AfWWoV"
      },
      "source": [
        "# Ploting helper functions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h7GesN6N3Hz"
      },
      "source": [
        "def plot_confueion_matrix(y_test , y_pred , title = \"Confusion Matrix\"):  \r\n",
        "  classes = np.array((\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"))\r\n",
        "\r\n",
        "  cmap = plt.cm.Blues\r\n",
        "  cm = confusion_matrix(y_test , y_pred)\r\n",
        "  \r\n",
        "  #plot the cm\r\n",
        "  plt.figure(figsize=(7,7))\r\n",
        "  plt.imshow(cm , interpolation='nearest' , cmap= cmap)\r\n",
        "  plt.colorbar()\r\n",
        "  thresh = cm.min() + (cm.max() - cm.min()) / 2.\r\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
        "        plt.text(j,i, cm[i, j],\r\n",
        "                 horizontalalignment=\"center\",\r\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "\r\n",
        "  tick_marks = np.arange(len(classes))\r\n",
        "  plt.xticks(tick_marks, classes, rotation=45,fontsize=10)\r\n",
        "  plt.yticks(tick_marks, classes,fontsize = 10)\r\n",
        "\r\n",
        "  plt.title(title)\r\n",
        "  plt.xlabel('Predicted value')\r\n",
        "  plt.ylabel('True values')\r\n",
        "\r\n",
        "  plt.tight_layout()\r\n",
        "  plt.show()\r\n",
        "  #plt.save('path.ghb.png')\r\n",
        "\r\n",
        "\r\n",
        "def plot_accuracy_loss_graph(result):\r\n",
        "  \r\n",
        "  plt.figure(figsize=(20,8))\r\n",
        "  plt.subplot(1,2,1)\r\n",
        "\r\n",
        "  plt.plot(result.history['accuracy'])\r\n",
        "  plt.plot(result.history['val_accuracy'])\r\n",
        "  plt.title('VGG Model Accuracy')\r\n",
        "  plt.ylabel('Accuracy')\r\n",
        "  plt.xlabel('Epoch')\r\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "  \r\n",
        "  #plt.savefig('ResNet Model Loss.png')\r\n",
        "\r\n",
        "  plt.subplot(1,2,2)\r\n",
        "\r\n",
        "  plt.plot(result.history['loss'])\r\n",
        "  plt.plot(result.history['val_loss'])\r\n",
        "  plt.title('VGG  Model Loss')\r\n",
        "  plt.ylabel('Loss')\r\n",
        "  plt.xlabel('Epoch')\r\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "  plt.show()\r\n",
        "  #plt.savefig('ResNet Model Loss.png')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stzXpEIXWrRO"
      },
      "source": [
        "# model training ---->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbsyxka-WZvF"
      },
      "source": [
        "#building the model\r\n",
        "def net(input_shape , num_classes , ):\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Conv2D(64, (5, 5), activation='relu', padding='same', input_shape=input_shape)) \r\n",
        "  model.add(Conv2D(64, (5, 5), activation='relu', padding='same', ))\r\n",
        "  model.add(BatchNormalization())\r\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "\r\n",
        "  #model.add(Conv2D(64, (5, 5), activation='relu', padding='same', ))\r\n",
        "  #model.add(BatchNormalization())\r\n",
        "  #model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "\r\n",
        "\r\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu',  padding='same', ))\r\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', padding='same',))\r\n",
        "  model.add(BatchNormalization())\r\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "\r\n",
        "\r\n",
        "  model.add(Conv2D(256, (3, 3), activation='relu', padding='same',))\r\n",
        "  model.add(Conv2D(256, (3, 3), activation='relu',  padding='same',))\r\n",
        "  model.add(BatchNormalization())\r\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "\r\n",
        "\r\n",
        "  model.add(Flatten())\r\n",
        "\r\n",
        "  model.add(Dense(1024, activation='relu',))\r\n",
        "  model.add(Dropout(0.2))\r\n",
        "  model.add(Dense(512, activation='relu', ))\r\n",
        "  model.add(Dropout(0.2))\r\n",
        "  model.add(Dense(128, activation='relu', ))\r\n",
        "  model.add(Dropout(0.2))\r\n",
        "  model.add(Dense(num_classes, activation='sigmoid'))\r\n",
        "\r\n",
        "\r\n",
        "  # compile model\r\n",
        "  \r\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "  #model summary\r\n",
        "  print(model.summary())\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERB37QbmW528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d018648-f0cc-4b8b-f4ae-111597a6791e"
      },
      "source": [
        "model = net(input_shape = (128,128,1), num_classes = 2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 128, 128, 64)      1664      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 128, 128, 64)      102464    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128, 128, 64)      256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 64, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64, 64, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 65536)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              67109888  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 68,913,218\n",
            "Trainable params: 68,912,322\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al8GGm_AXBfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371029a0-db44-4229-c545-7c6fa8b21a48"
      },
      "source": [
        "# set callbacks\r\n",
        "early_stoppping = EarlyStopping(monitor = 'val_loss',\r\n",
        "                                min_delta = 0.001,\r\n",
        "                                patience = 10,\r\n",
        "                                restore_best_weights=True)\r\n",
        "\r\n",
        "#set the global values\r\n",
        "epoches = 40\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "#fit the model\r\n",
        "history = model.fit(X_train, y_train,  \r\n",
        "          batch_size=batch_size,  \r\n",
        "          epochs=epoches,  \r\n",
        "          verbose=1,  \r\n",
        "          validation_data=(X_val, y_val),  \r\n",
        "          shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "20/20 [==============================] - 641s 32s/step - loss: 0.9775 - accuracy: 0.4616 - val_loss: 0.7020 - val_accuracy: 0.5238\n",
            "Epoch 2/40\n",
            "20/20 [==============================] - 630s 31s/step - loss: 0.6934 - accuracy: 0.5448 - val_loss: 0.8349 - val_accuracy: 0.4762\n",
            "Epoch 3/40\n",
            "20/20 [==============================] - 621s 31s/step - loss: 0.7628 - accuracy: 0.5069 - val_loss: 0.6941 - val_accuracy: 0.4762\n",
            "Epoch 4/40\n",
            "20/20 [==============================] - 617s 31s/step - loss: 0.6990 - accuracy: 0.5029 - val_loss: 0.6939 - val_accuracy: 0.4762\n",
            "Epoch 5/40\n",
            "20/20 [==============================] - 621s 31s/step - loss: 0.6953 - accuracy: 0.4767 - val_loss: 0.6934 - val_accuracy: 0.4762\n",
            "Epoch 6/40\n",
            "20/20 [==============================] - 616s 31s/step - loss: 0.6942 - accuracy: 0.4897 - val_loss: 0.6935 - val_accuracy: 0.4762\n",
            "Epoch 7/40\n",
            "20/20 [==============================] - 614s 31s/step - loss: 0.6935 - accuracy: 0.5013 - val_loss: 0.6954 - val_accuracy: 0.4762\n",
            "Epoch 8/40\n",
            "16/20 [=======================>......] - ETA: 2:04 - loss: 0.6932 - accuracy: 0.5015"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkfArrb7XXIB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}